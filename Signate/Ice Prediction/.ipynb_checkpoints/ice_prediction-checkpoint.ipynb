{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/0030/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/0030/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/0030/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/0030/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/0030/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/0030/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D,AveragePooling2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img, save_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img.MAX_IMAGE_PIXELS  =  1000000000\n",
    "Image.MAX_IMAGE_PIXELS  =  1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "im_width = 512\n",
    "im_height = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of images =  80\n"
     ]
    }
   ],
   "source": [
    "ids = next(os.walk(\"train_hh_images/\"))[2] # list of names all images in the given path\n",
    "print(\"No. of images = \", len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of images =  80\n"
     ]
    }
   ],
   "source": [
    "ids_ann = next(os.walk(\"train_annotations/\"))[2] # list of names all images in the given path\n",
    "print(\"No. of images = \", len(ids_ann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = sorted(ids)\n",
    "ids_ann = sorted(ids_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(ids), im_height, im_width, 1), dtype=np.float32)\n",
    "y = np.zeros((len(ids), im_height, im_width, 1), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 80 images.\n",
      "CPU times: user 30min 17s, sys: 48.9 s, total: 31min 6s\n",
      "Wall time: 1h 2min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for n, id_ in enumerate(zip(ids,ids_ann)):\n",
    "    # Load images\n",
    "    img = load_img(\"train_hh_images/\"+id_[0], color_mode = \"grayscale\")\n",
    "    x_img = img_to_array(img)\n",
    "    x_img = resize(x_img, (im_height, im_width, 1), mode = 'constant', preserve_range = True)\n",
    "    # Load masks\n",
    "    mask = img_to_array(load_img(\"train_annotations/\"+id_[1], color_mode = \"grayscale\"))\n",
    "    mask = resize(mask, (im_height, im_width,1), mode = 'constant', preserve_range = True)\n",
    "    # Save images\n",
    "    X[n] = x_img/255.0\n",
    "    y[n] = mask\n",
    "#     if n == 1:\n",
    "#         break\n",
    "print('Loaded {} images.'.format(n+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80, 256, 256, 13), (80, 256, 256, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80, 256, 256, 13), (80, 256, 256, 13))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and valid\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((72, 256, 256, 13), (8, 256, 256, 13), (72, 256, 256, 13), (8, 256, 256, 13))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
    "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "    # first layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor) #kernel_initializer = 'he_normal'\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # second layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet(input_img, n_filters = 32, dropout = 0.1, batchnorm = True):\n",
    "    \"\"\"Function to define the UNET Model\"\"\"\n",
    "    # Contracting Path\n",
    "    c1 = conv2d_block(input_img, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    p1 = Dropout(dropout)(p1)\n",
    "    \n",
    "    c2 = conv2d_block(p1, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    p2 = Dropout(dropout)(p2)\n",
    "    \n",
    "    c3 = conv2d_block(p2, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "    p3 = Dropout(dropout)(p3)\n",
    "    \n",
    "    c4 = conv2d_block(p3, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "    p4 = Dropout(dropout)(p4)\n",
    "    \n",
    "    c5 = conv2d_block(p4, n_filters = n_filters * 16, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    # Expansive Path\n",
    "    u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    u6 = Dropout(dropout)(u6)\n",
    "    c6 = conv2d_block(u6, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    u7 = Dropout(dropout)(u7)\n",
    "    c7 = conv2d_block(u7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    u8 = Dropout(dropout)(u8)\n",
    "    c8 = conv2d_block(u8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c8)\n",
    "    u9 = concatenate([u9, c1])\n",
    "    u9 = Dropout(dropout)(u9)\n",
    "    c9 = conv2d_block(u9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    outputs = Conv2D(13, (1, 1), activation='sigmoid')(c9)\n",
    "    model = Model(inputs=[input_img], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input((im_height, im_width, 1), name='img')\n",
    "model = get_unet(input_img, n_filters=32, dropout=0.1, batchnorm=True)\n",
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "img (InputLayer)                (None, 512, 512, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 512, 512, 32) 320         img[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 512, 512, 32) 128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 512, 512, 32) 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 256, 256, 32) 0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 256, 256, 32) 0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 256, 256, 64) 18496       dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 256, 256, 64) 256         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 256, 256, 64) 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 128, 128, 64) 0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 128, 128, 64) 0           max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 128, 128, 128 73856       dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 128, 128, 128 512         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 128, 128, 128 0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 64, 64, 128)  0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 64, 64, 128)  0           max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 64, 64, 256)  295168      dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 64, 64, 256)  1024        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 64, 64, 256)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 32, 32, 256)  0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 32, 32, 256)  0           max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 32, 32, 512)  1180160     dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 32, 32, 512)  2048        conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 32, 32, 512)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 64, 64, 256)  1179904     activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 64, 64, 512)  0           conv2d_transpose_5[0][0]         \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 64, 64, 512)  0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 64, 64, 256)  1179904     dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 64, 64, 256)  1024        conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 64, 64, 256)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTrans (None, 128, 128, 128 295040      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 128, 128, 256 0           conv2d_transpose_6[0][0]         \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 128, 128, 256 0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 128, 128, 128 295040      dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 128, 128, 128 512         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 128, 128, 128 0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTrans (None, 256, 256, 64) 73792       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256, 256, 128 0           conv2d_transpose_7[0][0]         \n",
      "                                                                 activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 256, 256, 128 0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 256, 256, 64) 73792       dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 256, 256, 64) 256         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 256, 256, 64) 0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTrans (None, 512, 512, 32) 18464       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 512, 512, 64) 0           conv2d_transpose_8[0][0]         \n",
      "                                                                 activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 512, 512, 64) 0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 512, 512, 32) 18464       dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 512, 512, 32) 128         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 512, 512, 32) 0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 512, 512, 13) 429         activation_36[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 4,708,717\n",
      "Trainable params: 4,705,773\n",
      "Non-trainable params: 2,944\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=20, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.0001, verbose=1),\n",
    "    ModelCheckpoint('model-tgs-classes.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.fit(X_train, y_train, batch_size=4, epochs=50, callbacks=callbacks,\\\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(results.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(results.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot( np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"log_loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model\n",
    "model.load_weights('model-tgs-classes.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set (this must be equals to the best log_loss)\n",
    "model.evaluate(X_valid, y_valid, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 600s 8s/step\n",
      "8/8 [==============================] - 9s 1s/step\n"
     ]
    }
   ],
   "source": [
    "# Predict on train, val and test\n",
    "preds_train = model.predict(X_train, verbose=1)\n",
    "preds_val = model.predict(X_valid, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of images =  40\n"
     ]
    }
   ],
   "source": [
    "ids = next(os.walk(\"test_hh_images/\"))[2] # list of names all images in the given path\n",
    "print(\"No. of images = \", len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of images =  40\n"
     ]
    }
   ],
   "source": [
    "id_hh = next(os.walk(\"test_hh_images/\"))[2] # list of names all images in the given path\n",
    "id_hv = next(os.walk(\"test_hv_images/\"))[2] # list of names all images in the given path\n",
    "print(\"No. of images = \", len(id_hh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_hh = sorted(id_hh)\n",
    "id_hv = sorted(id_hv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.zeros((len(id_hh), im_height, im_width, 1), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_shapes = [x for x in range(0,40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for n, id_ in tqdm(enumerate(id_hh)):\n",
    "    # Load images\n",
    "    img = load_img(\"test_hh_images/\"+id_, color_mode = \"grayscale\")\n",
    "    x_img = img_to_array(img)\n",
    "    X_shapes[n] = x_img.shape[:-1]\n",
    "    x_img = resize(x_img, (im_width , im_height, 1), mode = 'constant', preserve_range = True)\n",
    "    X1 = x_img/255.0\n",
    "    X_test[n] = X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model\n",
    "model.load_weights('model-tgs-classes.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 77s 2s/step\n"
     ]
    }
   ],
   "source": [
    "X_preds = model.predict(X_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_preds = np.argmax(X_preds, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for n, s in tqdm(enumerate(X_shapes)):\n",
    "  x1 = X_preds[n]\n",
    "  x1 = np.round(resize(x1,(s[0], s[1],1), mode = 'constant', preserve_range = True))\n",
    "  x1.reshape(s[0], s[1])\n",
    "  x1[(x1>0) & (x1<11)]=1\n",
    "  x1[(x1==0) |(x1>10)]=0\n",
    "  cv2.imwrite('test_annotations/test_{}.png'.format(n if n>9 else str(0) +str(n)),x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import cv2\n",
    "def make_json(categories):\n",
    "    count = 0\n",
    "    annotation_files = next(os.walk(\"test_annotations/\"))[2]\n",
    "    json_data = {}\n",
    "    s = time.clock()\n",
    "    for annotation_file in annotation_files:\n",
    "        print(annotation_file)\n",
    "        name = annotation_file.split('.')[0]\n",
    "        json_data[name] = {}\n",
    "        img = cv2.imread(os.path.join(\"test_annotations/\", annotation_file), flags = cv2.IMREAD_UNCHANGED)\n",
    "        for category in categories:\n",
    "            category_segments = {}\n",
    "            x, y = np.where(img==categories[category])\n",
    "            category_pix = {}\n",
    "            for i, j in zip(x, y):\n",
    "                if i not in category_pix:\n",
    "                    category_pix[i] = []\n",
    "                category_pix[i].append(j)\n",
    "            for l in category_pix:\n",
    "                segments = []\n",
    "                num_segments = 0\n",
    "                for i,v in enumerate(sorted(category_pix[l])):\n",
    "                    if i == 0:\n",
    "                        start = v\n",
    "                        end = v\n",
    "                    else:\n",
    "                        if v == end + 1:\n",
    "                            end = v\n",
    "                        else:\n",
    "                            segments.append([int(start), int(end)])\n",
    "                            start = v\n",
    "                            end = v\n",
    "                            num_segments += 1\n",
    "                segments.append([int(start), int(end)])\n",
    "                category_segments[int(l)]=segments\n",
    "            if len(category_pix):\n",
    "                json_data[name][category]=category_segments\n",
    "        count+=1\n",
    "        print(count, time.clock()-s)\n",
    "\n",
    "    return json_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "categories = {'ice':1}\n",
    "    \n",
    "json_data = make_json(categories)\n",
    "\n",
    "with open('submit.json', 'w') as f:\n",
    "    json.dump(json_data, f, sort_keys=True,separators=(',', ':'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
